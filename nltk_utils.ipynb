{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, string\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()                   # Import stemmer sama punctuation\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)             # Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    return stemmer.stem(word.lower())               # Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tokenized_sentence, list_words):\n",
    "    tokenized_sentence = [stem(w) for w in tokenized_sentence]             \n",
    "    bag = np.zeros(len(list_words), dtype=np.float32)\n",
    "\n",
    "    for index, w in enumerate(list_words):                                  # Initialize vocab berdasarkan dataset\n",
    "        if w in tokenized_sentence:\n",
    "            bag[index] = 1.0\n",
    "    \n",
    "    return bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('intents.json', 'r') as f:\n",
    "    intents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words = []\n",
    "tags = []\n",
    "responses = {}\n",
    "xy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents['intents']:                               # for every intents\n",
    "    tag = intent['tag']                                      \n",
    "    tags.append(tag)                                            # store every tags\n",
    "\n",
    "    for pattern in intent['patterns']:\n",
    "        words = tokenize(pattern)\n",
    "        list_words.extend(words)                                # store all words \n",
    "        xy.append((words, tag))                                 # dataset (all tokenized words and their pattern's label)\n",
    "    \n",
    "    for response in intent['responses']:\n",
    "        if tag in responses:\n",
    "            responses[tag].append(response)\n",
    "        else:\n",
    "            responses[tag] = [response]\n",
    "\n",
    "list_words = [stem(w) for w in list_words if w not in punctuations]\n",
    "list_words = sorted(set(list_words))\n",
    "tags = sorted(set(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['food', 'goodbye', 'greeting', 'thanks', 'weather']\n"
     ]
    }
   ],
   "source": [
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Hi'], 'greeting'), (['Hey'], 'greeting'), (['How', 'are', 'you'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['Bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['Goodbye', 'now'], 'goodbye'), (['Goodbye', 'now', 'see', 'you', 'later'], 'goodbye'), (['What', \"'s\", 'the', 'weather', 'like', 'today', '?'], 'weather'), (['How', \"'s\", 'the', 'weather', 'looking', 'for', 'today', '?'], 'weather'), (['What', \"'s\", 'the', 'temperature', 'outside', '?'], 'weather'), (['Is', 'it', 'going', 'to', 'rain', 'today', '?'], 'weather'), (['Can', 'you', 'recommend', 'a', 'good', 'food', 'to', 'eat', '?'], 'food'), (['What', \"'s\", 'a', 'good', 'dish', 'to', 'try', '?'], 'food'), (['What', \"'s\", 'the', 'best', 'food', 'to', 'eat', 'around', 'here', '?'], 'food'), (['I', \"'m\", 'a', 'bit', 'hungry'], 'food'), (['Thanks'], 'thanks'), (['Thank', 'you'], 'thanks'), (['That', \"'s\", 'helpful'], 'thanks'), (['Thank', \"'s\", 'a', 'lot', '!'], 'thanks')]\n"
     ]
    }
   ],
   "source": [
    "print(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'greeting': ['Hey :-)', 'Hello, thanks for visiting', 'Hi there, what can I do for you?', 'Hi there, how can I help?'], 'goodbye': ['See you later, thanks for visiting', 'Have a nice day', 'Bye! Come back again soon.'], 'weather': ['Currently, the weather is overcast and the temperature is 30°C', \"Right now in Jakarta, it's overcast and 30°C\", \"It's currently 30°C in Jakarta.\"], 'food': [\"How about trying the sate? It's a popular choice.\", 'You should try eating sate, they are excellent.', 'For Indonesian food, I recommend sate. They are great for any meal of the day'], 'thanks': ['Happy to help!', 'Any time!', 'My pleasure']}\n"
     ]
    }
   ],
   "source": [
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y= []\n",
    "\n",
    "for (tokenized_sentence, tag) in xy:  \n",
    "    bag = bag_of_words(tokenized_sentence, list_words)                # create [0 1 0 1 0] array by matching tokenized words with the one at our list_words\n",
    "    X.append(bag) \n",
    "\n",
    "    y.append(tag)  \n",
    "\n",
    "X = np.array(X)                                                        # Initialize x and y \n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def train():\n",
    "    global model\n",
    "    if os.path.exists('model.pickle'):\n",
    "        with open('model.pickle', 'rb') as model_file:\n",
    "            model = pickle.load(model_file)\n",
    "\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        model = MultinomialNB()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        with open('model.pickle', 'wb') as model_file:\n",
    "            pickle.dump(model, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_random_response(class_label):\n",
    "    if class_label in responses:\n",
    "        return random.choice(responses[class_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_menu(): \n",
    "    while True:\n",
    "        sentence = str(input(\">>\"))\n",
    "        if ((sentence == 'Quit') | (sentence == 'quit')):\n",
    "            break \n",
    "        else:\n",
    "            global model\n",
    "            X_sentence = [bag_of_words(tokenize(sentence), list_words)]\n",
    "            probabilities = model.predict_proba(X_sentence)[0]\n",
    "            print(probabilities)\n",
    "            max_proba = np.max(probabilities)\n",
    "\n",
    "            y_pred = model.predict(X_sentence)[0]\n",
    "\n",
    "            if (max_proba < 0.35):\n",
    "                print(\"Sorry I'm not sure I know how to answer that\")\n",
    "                print()\n",
    "            else:\n",
    "                response = get_random_response(y_pred)\n",
    "                print(response)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train()\n",
    "    main_menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01428236 0.11443119 0.76287458 0.07628746 0.03212441]\n",
      "Hey :-)\n",
      "\n",
      "[0.01836432 0.00972451 0.01215564 0.00972451 0.95003102]\n",
      "Currently, the weather is overcast and the temperature is 30°C\n",
      "\n",
      "[0.39542546 0.22225987 0.18521656 0.14817325 0.04892485]\n",
      "For Indonesian food, I recommend sate. They are great for any meal of the day\n",
      "\n",
      "[0.05391031 0.01698071 0.02122589 0.01698071 0.89090237]\n",
      "It's currently 30°C in Jakarta.\n",
      "\n",
      "[0.02533832 0.12075729 0.10063107 0.64403888 0.10923444]\n",
      "My pleasure\n",
      "\n",
      "[0.00188295 0.96916639 0.01495627 0.01196502 0.00202937]\n",
      "Bye! Come back again soon.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
